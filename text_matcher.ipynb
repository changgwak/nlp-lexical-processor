{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['major/minor', 'graduates/post-graduates', 'course outline/syllabus', 'conditional offer', 'admissions status - regular/transfer/visitor/no credit', 'elective', 'restriction (against a course)', 'withdrawal', 'regular session/semester', 'drop and add', 'tuition', 'job qualification', 'upper-division/lower-division', 'summer session/semester', 'quarter system', 'summer job/position', 'non-credit course', 'academic advisor/counselor', 'pay check', 'placement test', 'career day/job fair', 'honors program', 'department', 'citation', 'acceptance/admission', 'accreditation', 'being on contract', 'disenrollment/drop-out', 'degree requirements/graduation requirements']\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "# Let's create a function to check if all provided words are included in the text\n",
    "\n",
    "import random\n",
    "import spacy\n",
    "import pyinflect\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Ensure the required nltk data is downloaded\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_word_forms_(word):\n",
    "    words = classify_words(word)\n",
    "\n",
    "    return words\n",
    "\n",
    "def classify_words(word):\n",
    "    noun_forms = None\n",
    "    verb_forms = None\n",
    "    adjective_forms = None\n",
    "    adverb_forms = None\n",
    "\n",
    "    if is_adjective(word):\n",
    "        adjective_forms = get_list_forms(word, 'JJ')\n",
    "    \n",
    "    if is_adverb(word):\n",
    "        adverb_forms = get_list_forms(word, 'RB')\n",
    "\n",
    "    if is_noun(word):\n",
    "        noun_forms = get_list_noun_forms(word)\n",
    "    \n",
    "    if is_verb(word):\n",
    "        verb_forms = get_list_verb_forms(word)\n",
    "\n",
    "    return noun_forms, verb_forms, adjective_forms, adverb_forms\n",
    "\n",
    "def is_noun(word):\n",
    "    # Check if the word has any noun synsets in WordNet\n",
    "    return any(ss.pos() == 'n' for ss in wn.synsets(word))\n",
    "\n",
    "def is_verb(word):\n",
    "    # Check if the word has any verb synsets in WordNet\n",
    "    return any(ss.pos() == 'v' for ss in wn.synsets(word))\n",
    "\n",
    "def is_adjective(word):\n",
    "    # Check if the word has any adjective synsets in WordNet\n",
    "    return any(ss.pos() == 'a' or ss.pos() == 's' for ss in wn.synsets(word))\n",
    "\n",
    "def is_adverb(word):\n",
    "    # Check if the word has any adverb synsets in WordNet\n",
    "    return any(ss.pos() == 'r' for ss in wn.synsets(word))\n",
    "\n",
    "\n",
    "def get_list_forms(word, pos):\n",
    "    doc = nlp(word)\n",
    "    token = doc[0]\n",
    "\n",
    "    base_form = token.text\n",
    "    comparative_form = token._.inflect(f'JJR' if pos == 'JJ' else 'RBR') if token._.inflect(f'JJR' if pos == 'JJ' else 'RBR') else word\n",
    "    superlative_form = token._.inflect(f'JJS' if pos == 'JJ' else 'RBS') if token._.inflect(f'JJS' if pos == 'JJ' else 'RBS') else word\n",
    "    \n",
    "    return base_form, comparative_form, superlative_form\n",
    "    \n",
    "\n",
    "def get_list_noun_forms(word):\n",
    "    doc = nlp(word)\n",
    "    token = doc[0]\n",
    "\n",
    "    singular_form = token._.inflect('NN') if token._.inflect('NN') else word\n",
    "    plural_form = token._.inflect('NNS') if token._.inflect('NNS') else word\n",
    "    \n",
    "    return singular_form, plural_form\n",
    "    \n",
    "def get_list_verb_forms(word):\n",
    "    if word.lower() == \"be\":\n",
    "        return 'be', 'was', 'were', 'being', 'been', 'am', 'is', 'are'\n",
    "        \n",
    "    base_form = lemmatizer.lemmatize(word, 'v')\n",
    "    \n",
    "    return base_form,conjugate_verb(base_form, 'VBD'),conjugate_verb(base_form, 'VBG'),conjugate_verb(base_form, 'VBN'),base_form, conjugate_verb(base_form, 'VBZ')\n",
    "\n",
    "def get_list_idiom_forms(word):\n",
    "    \n",
    "    first_word = word.split()[0]\n",
    "    rest_of_words = ' '.join(word.split()[1:])\n",
    "\n",
    "    if first_word.lower() == \"be\":\n",
    "        return 'be'+' '+ rest_of_words, 'was'+' '+ rest_of_words, 'were'+' '+ rest_of_words, 'being'+' '+ rest_of_words, 'been'+' '+ rest_of_words, 'am'+' '+ rest_of_words, 'is'+' '+ rest_of_words, 'are'+' '+ rest_of_words\n",
    "\n",
    "    base_form = lemmatizer.lemmatize(first_word, 'v')\n",
    "    return base_form +' '+ rest_of_words,conjugate_verb(base_form, 'VBD')+' '+ rest_of_words,conjugate_verb(base_form, 'VBG')+' '+ rest_of_words,conjugate_verb(base_form, 'VBN')+' '+ rest_of_words,base_form+' '+ rest_of_words, conjugate_verb(base_form, 'VBZ')+' '+ rest_of_words\n",
    "\n",
    "def conjugate_verb(base, tense):\n",
    "    doc = nlp(base)\n",
    "    token = doc[0]\n",
    "    if tense == 'VBD':\n",
    "        return token._.inflect(\"VBD\")\n",
    "    elif tense == 'VBG':\n",
    "        return token._.inflect(\"VBG\")\n",
    "    elif tense == 'VBN':\n",
    "        return token._.inflect(\"VBN\")\n",
    "    elif tense == 'VBZ':\n",
    "        return token._.inflect(\"VBZ\")\n",
    "    return base\n",
    "\n",
    "\n",
    "def convert_to_unique_list(input_str):\n",
    "    \"\"\"\n",
    "    Converts a string representation of a list of tuples to a list of unique values,\n",
    "    removing any 'None' values.\n",
    "    \n",
    "    Args:\n",
    "        input_str (str): The input string to be processed.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of unique values.\n",
    "    \"\"\"\n",
    "    # Remove the parentheses and quotes\n",
    "    cleaned_str = re.sub(r'[()\"]', '', input_str)\n",
    "    # Split the string into a list of tuples\n",
    "    tuples = [tuple(x.strip().split(', ')) for x in cleaned_str.split('), (')]\n",
    "    # print(\"tuples1: \", tuples)\n",
    "    \n",
    "    # Flatten the list of tuples and remove duplicates\n",
    "    result = list(set([item for tup in tuples for item in tup]))\n",
    "    \n",
    "    # Remove any 'None' values\n",
    "    result = [x for x in result if x != 'None']\n",
    "\n",
    "    result = [item.strip(\"'\") for item in result]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def string_to_random_list(input_string):\n",
    "    \"\"\"\n",
    "    Takes a comma-separated string as input and returns a list of strings in random order.\n",
    "    \"\"\"\n",
    "    word_list = [word.strip() for word in input_string.split(',')]\n",
    "    random.shuffle(word_list)\n",
    "    return list(set(word_list))\n",
    "\n",
    "def check_words_in_text(text, words):\n",
    "    if type(words) is not list:\n",
    "        words = string_to_random_list(words)\n",
    "        \n",
    "    # Convert both text and words to lowercase\n",
    "    text = text.lower()\n",
    "    words = [word.lower() for word in words]    \n",
    "    \n",
    "    # missing_words = [word for word in words if word not in text]\n",
    "    \n",
    "    for word in words :\n",
    "        word_forms = get_word_forms_(word) # from word\n",
    "        word_forms_list = convert_to_unique_list(str(word_forms))\n",
    "        if len(word_forms_list) == 0:\n",
    "            idioms_forms = None\n",
    "            word_forms_list.append(word)\n",
    "            if is_verb(word.split()[0]): \n",
    "                idioms_forms = list(get_list_idiom_forms(word))\n",
    "                for idiom in idioms_forms:\n",
    "                    word_forms_list.append(idiom)\n",
    "                word_forms_list= list(set(word_forms_list))\n",
    "        word_flag = False\n",
    "        for each_word in word_forms_list:\n",
    "            if each_word in text:\n",
    "                word_flag = True\n",
    "        if word_flag == True: pass\n",
    "        elif word_flag == False:\n",
    "            missing_words.append(word) \n",
    "\n",
    "    return missing_words\n",
    "\n",
    "\n",
    "words_list = None\n",
    "story_text = \"\"\n",
    "missing_words = []\n",
    "\n",
    "# Provided words list\n",
    "words_list = ['major/minor', 'office hours', 'graduates/post-graduates', 'participation', 'course outline/syllabus', 'getting a raise', 'temporary job', 'conditional offer', 'certificate programs', 'take-home exam', 'make-up exam', 'admissions status - regular/transfer/visitor/no credit', 'elective', 'open-book exam', 'restriction (against a course)', 'withdrawal', 'regular session/semester', 'independent study', 'permanent position', 'fees', 'extra-curricular activities', 'drop and add', 'tuition', 'job qualification', 'academic probation', 'upper-division/lower-division', 'summer session/semester', 'quarter system', 'summer job/position', 'non-credit course', 'office clerk', 'academic advisor/counselor', 'pay check', 'placement test', 'career day/job fair', 'academic session', 'fast-track class', 'academic tutor', 'honors program', 'department', 'citation', 'faculty', 'acceptance/admission', 'accreditation', 'grade point average (gpa)', 'being on contract', 'disenrollment/drop-out', 'degree requirements/graduation requirements', 'audit']\n",
    "\n",
    "# Given story text\n",
    "story_text = \"\"\"\n",
    "Scene 1: Emily meets Dr. Smith for Academic Advising\n",
    "\n",
    "Emily: Hi Dr. Smith, I’m Emily. I’m here for advice on my major and minor choices.\n",
    "\n",
    "Dr. Smith: Hi Emily. Have you decided on your major yet?\n",
    "\n",
    "Emily: I’m leaning towards a major in Psychology and a minor in Sociology.\n",
    "\n",
    "Dr. Smith: That sounds like a solid combination. Make sure to check the course outline and syllabus for each class. Some courses might have restrictions.\n",
    "\n",
    "Emily: I will. I also have a question about office hours. When are yours?\n",
    "\n",
    "Dr. Smith: My office hours are posted outside my office, but generally, they are Monday and Wednesday afternoons.\n",
    "\n",
    "Emily: Great, I’ll make sure to visit if I have more questions. By the way, what’s the admissions status of a transfer student?\n",
    "\n",
    "Dr. Smith: A transfer student is someone who comes from another institution with some credits already completed. They need to check if those credits meet our degree requirements.\n",
    "\n",
    "Scene 2: John, a Recent Graduate, Talks about His Experience\n",
    "\n",
    "Emily: Hi John, I heard you just graduated. Congratulations!\n",
    "\n",
    "John: Thanks, Emily! It feels great to finally be done.\n",
    "\n",
    "Emily: Can you tell me about your experience with the graduation requirements?\n",
    "\n",
    "John: Sure. Meeting the degree requirements was tough. I had to maintain a good grade point average (GPA) and complete both upper-division and lower-division courses.\n",
    "\n",
    "Emily: Did you take any certificate programs?\n",
    "\n",
    "John: Yes, I took a couple of certificate programs in addition to my major. It added some fees, but it was worth it.\n",
    "\n",
    "Emily: What was the most challenging part?\n",
    "\n",
    "John: For me, the open-book exams and take-home exams were challenging because they required a lot of independent study. But participation in extra-curricular activities helped balance things out.\n",
    "\n",
    "Scene 3: Lisa, the Part-time Office Clerk, Balances Work and Study\n",
    "\n",
    "Emily: Hi Lisa, how do you manage your job as an office clerk with your studies?\n",
    "\n",
    "Lisa: Hi Emily. It’s all about time management. I work part-time and try to schedule my classes in the morning.\n",
    "\n",
    "Emily: Are you planning to get a permanent position after you graduate?\n",
    "\n",
    "Lisa: Yes, I’m hoping to. Right now, I’m on a temporary job contract, but I’m gaining valuable experience.\n",
    "\n",
    "Emily: What about getting a raise? Is that possible in your current role?\n",
    "\n",
    "Lisa: It is, but it depends on my performance during the probationary period and the qualifications I bring to the job.\n",
    "\n",
    "Scene 4: Discussion about Exams and Academic Policies\n",
    "\n",
    "Emily: Dr. Smith, I have a question about make-up exams. What if I miss an exam?\n",
    "\n",
    "Dr. Smith: If you miss an exam, you need to notify the faculty as soon as possible. We can arrange a make-up exam under certain conditions.\n",
    "\n",
    "Emily: What about if I want to audit a class?\n",
    "\n",
    "Dr. Smith: Auditing a class means you attend it without receiving credit. It’s a good option if you’re interested in the subject but don’t need the credit.\n",
    "\n",
    "Emily: I’m also considering a fast-track class during the summer session. Do you think that’s a good idea?\n",
    "\n",
    "Dr. Smith: Fast-track classes are intensive, but they can help you catch up or get ahead. Just make sure it fits into your academic session plan.\n",
    "\n",
    "Scene 5: Career Day and Job Fair Preparation\n",
    "\n",
    "Emily: Lisa, are you going to the career day and job fair?\n",
    "\n",
    "Lisa: Yes, I am. It’s a great opportunity for campus recruitment. Plus, they often have mock interviews which are really helpful.\n",
    "\n",
    "Emily: That sounds useful. I’m also looking for a summer job or position related to my major.\n",
    "\n",
    "Lisa: Definitely attend. They have various employers looking for both temporary and permanent positions.\n",
    "\n",
    "Scene 6: Discussing Academic Challenges\n",
    "\n",
    "Emily: Dr. Smith, what happens if someone is on academic probation?\n",
    "\n",
    "Dr. Smith: If a student’s GPA falls below the required level, they are placed on academic probation. They must improve their grades in the following semester to avoid disenrollment or drop-out.\n",
    "\n",
    "Emily: That sounds serious. I’ll make sure to keep my grades up.\n",
    "\n",
    "Dr. Smith: Good idea. Remember, you can always seek help from an academic tutor or counselor if you need it.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# story_text = \"The quick brown fox jumps over the lazy dog. It give off, are made up of, **sprang up** and **comprised**\"\n",
    "# words_list = \"Spring up, Comprise, Flight, Cat, Dog, give off, be made up of\"\n",
    "\n",
    "\n",
    "# Check for missing words\n",
    "missing_words = check_words_in_text(story_text, words_list)\n",
    "print(missing_words)\n",
    "print(len(missing_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made up of\n",
      "be made up of\n",
      "['is made up of', 'being made up of', 'was made up of', 'be made up of', 'been made up of']\n"
     ]
    }
   ],
   "source": [
    "word='be made up of'\n",
    "word_forms_list=[]\n",
    "# if len(word_forms_list) ==0:\n",
    "#     word_forms_list.append(word)\n",
    "first_word = word.split()[0]\n",
    "rest_of_words = ' '.join(word.split()[1:])\n",
    "print(rest_of_words)\n",
    "print(first_word + ' '+ rest_of_words)\n",
    "\n",
    "if len(word_forms_list) == 0:\n",
    "    idioms_forms = None\n",
    "    word_forms_list.append(word)\n",
    "    if is_verb(word.split()[0]): \n",
    "        idioms_forms = list(get_list_idiom_forms(word))\n",
    "        for idiom in idioms_forms:\n",
    "            word_forms_list.append(idiom)\n",
    "        word_forms_list= list(set(word_forms_list))\n",
    "\n",
    "print(word_forms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be made up of', 'tremulous', 'calculatedly', 'trembling', 'come before', 'appoint', 'presume', 'accept as true', 'inducement', 'meander', 'specify', 'pine', 'feel compassion', 'idiosyncrasy', '(sudden) advance', 'relinquish', 'departure', 'solitary', 'wind']\n"
     ]
    }
   ],
   "source": [
    "# Let's create a function to check if all provided words are included in the text\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def string_to_random_list(input_string):\n",
    "    \"\"\"\n",
    "    Takes a comma-separated string as input and returns a list of strings in random order.\n",
    "    \"\"\"\n",
    "    word_list = [word.strip() for word in input_string.split(',')]\n",
    "    random.shuffle(word_list)\n",
    "    return list(set(word_list))\n",
    "\n",
    "def check_words_in_text(text, words):\n",
    "    if type(words) is not list:\n",
    "        words = string_to_random_list(words)\n",
    "        \n",
    "    # Convert both text and words to lowercase\n",
    "    text = text.lower()\n",
    "    words = [word.lower() for word in words]    \n",
    "    \n",
    "    missing_words = [word for word in words if word not in text]\n",
    "\n",
    "    return missing_words\n",
    "\n",
    "\n",
    "words_list = None\n",
    "story_text = \"\"\n",
    "missing_words = []\n",
    "\n",
    "# Provided words list\n",
    "words_list = ['hairline', 'likely', 'be made up of', 'loyalty', 'tremulous', 'make attractive', 'intentionally', 'incitement', 'sympathize', 'calculatedly', 'diminutive', 'trembling', 'come before', 'irremediable', 'appoint', 'distribute', 'keep in check', 'snake', 'presume', 'primarily', 'accept as true', 'eccentricity', 'rule out', 'allegiance', 'coarse', 'inducement', 'accustomed', 'yearn', 'shrewd', 'be composed of', 'aberration', 'ornament', 'meander', 'rebellion', 'incident', 'come out', 'terminate', 'specify', 'pine', 'feel compassion', 'idiosyncrasy', 'devise', 'revolt', '(sudden) advance', 'trace', 'sole', 'relinquish', 'departure', 'solitary', 'wind', 'divergence', 'trustworthy', 'irreversible']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Given story text\n",
    "story_text = \"\"\"\n",
    "### The Evolution of Robotics and Artificial Intelligence\n",
    "\n",
    "The field of robotics and artificial intelligence (AI) has experienced a **sudden advance** over the past few decades. This **phenomenon** can be **traced** back to numerous incremental improvements, **primarily** driven by innovations in computing power and algorithmic efficiency. Modern robots **are made up of** sophisticated sensors and AI systems that enable them to perform tasks once thought to be the **sole** domain of humans.\n",
    "\n",
    "### Design and Structure\n",
    "\n",
    "Robots **be composed of** various components, each playing a crucial role in their functionality. These components include **hairline** precise circuits and **diminutive** yet powerful processors that **snake** through their bodies, enabling **shrewd** decision-making processes. The design is often **ornamented** to **make attractive** their appearance, making them more appealing to users.\n",
    "\n",
    "### Ethical and Social Implications\n",
    "\n",
    "The deployment of AI and robotics is not without its ethical dilemmas. One must **keep in check** the potential for misuse, which could lead to **irremediable** harm. Developers must **intentionally** design systems with ethical considerations, **calculatingly** weighing the benefits against the risks. For instance, AI systems must **rule out** biases and ensure fairness to garner public **trustworthy**.\n",
    "\n",
    "The relationship between humans and robots also **comes with** its peculiar **idiosyncrasies**. For example, there is a **yearning** among some for robots that can **sympathize** and understand human emotions. This desire highlights the **eccentricity** of human-robot interactions, where people often project their **loyalty** and **allegiance** onto machines.\n",
    "\n",
    "### Challenges and Adaptations\n",
    "\n",
    "One of the major challenges in the field is the potential for societal **rebellion** against widespread automation. This **rebellion** could be seen as an **aberration**, a **divergence** from the usual acceptance of technology. Incidents of **revolt** against robots, fueled by fears of job displacement, are **likely** to increase. It is essential to **devise** strategies to **distribute** the benefits of robotics evenly and ensure that the workforce is **accustomed** to new roles.\n",
    "\n",
    "Moreover, developers must be prepared to address any **incitement** to fear through transparent communication and ethical practices. As robots **come out** of the laboratories and into everyday life, there will be a period of adjustment, where old roles are **terminated** and new ones are created.\n",
    "\n",
    "### Future Prospects\n",
    "\n",
    "Looking forward, the future of robotics and AI appears **irreversible** in its trajectory. As technology continues to advance, robots will become more integrated into daily life. The **calculated** steps taken now will shape how society adapts to these changes. The goal is to create a harmonious coexistence where robots are seen not as **coarse** intruders but as **trustworthy** companions.\n",
    "\n",
    "In conclusion, the evolution of robotics and AI is a complex journey marked by innovation, ethical considerations, and societal impacts. By understanding and addressing the challenges, we can ensure a future where technology enhances human life while maintaining the delicate balance of our social fabric.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# story_text = \"The quick brown fox jumps over the lazy dog. It give off, **sprang up** and **comprised**\"\n",
    "# words_list = \"Spring up, Comprise, Flight, Cat, Dog, give off\"\n",
    "\n",
    "# Check for missing words\n",
    "missing_words = check_words_in_text(story_text, words_list)\n",
    "print(missing_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "19\n",
      "be made up of tremulous calculatedly trembling come before appoint presume accept as true inducement meander specify pine feel compassion idiosyncrasy (sudden) advance relinquish departure solitary wind\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Let's create a function to check if all provided words are included in the text\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def string_to_random_list(input_string):\n",
    "    \"\"\"\n",
    "    Takes a comma-separated string as input and returns a list of strings in random order.\n",
    "    \"\"\"\n",
    "    word_list = [word.strip() for word in input_string.split(',')]\n",
    "    random.shuffle(word_list)\n",
    "    return list(set(word_list))\n",
    "\n",
    "def check_words_in_text(text, words):\n",
    "    if type(words) is not list:\n",
    "        words = string_to_random_list(words)\n",
    "\n",
    "    if type(text) is list:\n",
    "        text = ' '.join(text)\n",
    "    print(text)\n",
    "    # Convert both text and words to lowercase\n",
    "    text = text.lower()\n",
    "    words = [word.lower() for word in words]    \n",
    "    \n",
    "    missing_words = [word for word in words if word not in text]\n",
    "\n",
    "    return missing_words\n",
    "\n",
    "\n",
    "words_list = None\n",
    "story_text = \"\"\n",
    "missing_words = []\n",
    "\n",
    "# Provided words list\n",
    "words_list = ['tremulous', 'calculatedly', 'trembling', 'come before', 'appoint', 'presume', 'accept as true', 'inducement', 'meander', 'specify', 'pine', 'feel compassion', '(sudden) advance', 'relinquish', 'departure', 'solitary', 'wind']\n",
    "\n",
    "print(len(words_list))\n",
    "\n",
    "# Given story text\n",
    "story_text = ['be made up of', 'tremulous', 'calculatedly', 'trembling', 'come before', 'appoint', 'presume', 'accept as true', 'inducement', 'meander', 'specify', 'pine', 'feel compassion', 'idiosyncrasy', '(sudden) advance', 'relinquish', 'departure', 'solitary', 'wind']\n",
    "\n",
    "print(len(story_text))\n",
    "# story_text = \"The quick brown fox jumps over the lazy dog. It **sprang up** and **comprised**\"\n",
    "# words_list = \"Spring up, Comprise, Flight, Cat, Dog\"\n",
    "\n",
    "# Check for missing words\n",
    "missing_words = check_words_in_text(story_text, words_list)\n",
    "print(missing_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['immense', 'disprove', 'atypically', 'chancy', 'primarily', 'preserve', 'valuable', 'perilous', 'discerning', 'praise', 'magnificence', 'tame', 'immersed in', 'sufficient', 'clement', 'moderate', 'refine', 'pronounced', 'extent', 'admire', 'allow', 'let', 'sue', 'oddly', 'apparatus', 'reproduce', 'breed', 'household', 'bias', 'frighten', 'distinct', 'scheme', 'remain', 'perceptive', 'helpful for', 'without planning', 'brew', 'novelty', 'probable', 'persist', 'flourish', 'uncovered', 'excessively', 'train', 'incite', 'in agreement with', 'commend', 'be on the way', 'precise', 'disintegration', 'colossal', 'congruent with', 'appliance', 'marked', 'bar', 'virtual', 'propagate', 'multiply', 'akin to', 'loom', 'chiefly', 'device', 'menace', 'compliment', 'favorable to', 'complement', 'disruption', 'equivalent', 'fleet', 'conducive to', 'believable', 'on impulse', 'succeed', 'exceedingly', 'fault', 'groundless', 'prove false', 'arouse', 'striking', 'upright', 'beneficial to', 'disregard', 'splendor', 'convert', 'belittle', 'intimidate', 'awake', 'principally', 'erect', 'counterpart', 'destruction', 'permit', 'tuned to', 'astute', 'imitate', 'underestimate', 'plausible', 'hardly perceived', 'fragmentation', 'swift', 'blemish', 'cushion', 'adequate', 'substitutive', 'baseless', 'dear', 'duplicate', 'renowned', 'go through', 'consistent with', 'absorbed in', 'refute', 'charge', 'enable', 'subtle', 'varying', 'divergent', 'exceptionally', 'carry on', 'alter', 'penetrate', 'up-and-down', 'engross', 'take a strong legal action against', 'administer', 'provoke', 'engage', 'stimulate', 'substitute', 'ample', 'threaten', 'unfounded', 'priceless', 'slight', 'defect', 'buffer', 'flaw', 'well-known', 'pierce', 'celebrated', 'temperate', 'prejudice', 'on the spur of the moment', 'noticeable', 'persevere', 'discern', 'preoccupied with', 'rebut', 'majesty', 'dissimilar', 'conduct', 'unexplored', 'dictate', 'consume', 'grandeur', 'mild', 'obstruction', 'endure', 'nonetheless', 'domesticate']\n",
      "161\n",
      "160\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def string_to_random_list(input_string):\n",
    "    \"\"\"\n",
    "    Takes a comma-separated string as input and returns a list of strings in random order.\n",
    "    \"\"\"\n",
    "    word_list = [word.strip() for word in input_string.split(',')]\n",
    "    random.shuffle(word_list)\n",
    "    return list(set(word_list))\n",
    "\n",
    "def get_count(input_list):\n",
    "    return len(input_list)\n",
    "\n",
    "def get_count_comma(input_string):\n",
    "    return input_string.count(\",\")\n",
    "\n",
    "input_string =''\n",
    "output_list = []\n",
    "\n",
    "input_string = 'plausible, believable, probable, convert, alter, counterpart, complement, equivalent, nonetheless, pronounced, striking, marked, noticeable, distinct, celebrated, renowned, well-known, exceedingly, excessively, chancy, perilous, upright, up-and-down, erect, persist, endure, remain, extent, discern, discerning, astute, perceptive, substitutive, substitute, scheme, refine, valuable, dear, priceless, precise, subtle, hardly perceived, slight, conducive to, favorable to, helpful for, beneficial to, preoccupied with, absorbed in, immersed in, swift, fleet, menace, threaten, intimidate, frighten, akin to, consume, belittle, disregard, underestimate, commend, admire, praise, compliment, dictate, sue, take a strong legal action against, charge, novelty, fragmentation, destruction, disintegration, disruption, principally, primarily, chiefly, sufficient, adequate, ample, oddly, exceptionally, atypically, bar, obstruction, immense, colossal, reproduce, duplicate, imitate, breed, multiply, propagate, allow, enable, permit, let, penetrate, pierce, go through, device, apparatus, appliance, divergent, varying, dissimilar, flaw, defect, fault, blemish, buffer, cushion, refute, disprove, rebut, prove false, household, domesticate, tame, train, on the spur of the moment, without planning, on impulse, temperate, moderate, mild, clement, consistent with, in agreement with, congruent with, splendor, magnificence, grandeur, majesty, succeed, flourish, brew, loom, be on the way, persevere, carry on, persist, preserve, bias, prejudice, administer, conduct, groundless, unfounded, baseless, arouse, stimulate, provoke, incite, awake, engross, engage, virtual, unexplored, uncovered, tuned to, in agreement with'\n",
    "\n",
    "output_list = string_to_random_list(input_string)\n",
    "print(output_list)\n",
    "print(get_count_comma(input_string))\n",
    "print(get_count(output_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('be', 'be'), ('be', 'was', 'being', 'been', 'be', 'is'), None, None)\n",
      "['being', 'was', 'is', 'been', 'be']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pyinflect\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Ensure the required nltk data is downloaded\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_word_forms_(word):\n",
    "    words = classify_words(word)\n",
    "    return words\n",
    "\n",
    "def get_word_forms(word):\n",
    "    noun_forms = None\n",
    "    verb_forms = None\n",
    "    adjective_forms = None\n",
    "    adverb_forms = None\n",
    "\n",
    "    if is_adjective(word):\n",
    "        adjective_forms = get_forms(word, 'JJ')\n",
    "    \n",
    "    if is_adverb(word):\n",
    "        adverb_forms = get_forms(word, 'RB')\n",
    "\n",
    "    if is_noun(word):\n",
    "        noun_forms = get_noun_forms(word)\n",
    "    \n",
    "    if is_verb(word):\n",
    "        verb_forms = get_verb_forms(word)\n",
    "\n",
    "    return {\n",
    "        'noun_forms': noun_forms,\n",
    "        'verb_forms': verb_forms,\n",
    "        'adjective_forms': adjective_forms,\n",
    "        'adverb_forms': adverb_forms\n",
    "    }\n",
    "\n",
    "\n",
    "def classify_words(word):\n",
    "    noun_forms = None\n",
    "    verb_forms = None\n",
    "    adjective_forms = None\n",
    "    adverb_forms = None\n",
    "\n",
    "    if is_adjective(word):\n",
    "        adjective_forms = get_list_forms(word, 'JJ')\n",
    "    \n",
    "    if is_adverb(word):\n",
    "        adverb_forms = get_list_forms(word, 'RB')\n",
    "\n",
    "    if is_noun(word):\n",
    "        noun_forms = get_list_noun_forms(word)\n",
    "    \n",
    "    if is_verb(word):\n",
    "        verb_forms = get_list_verb_forms(word)\n",
    "\n",
    "    return noun_forms, verb_forms, adjective_forms, adverb_forms\n",
    "\n",
    "def is_noun(word):\n",
    "    # Check if the word has any noun synsets in WordNet\n",
    "    return any(ss.pos() == 'n' for ss in wn.synsets(word))\n",
    "\n",
    "def is_verb(word):\n",
    "    # Check if the word has any verb synsets in WordNet\n",
    "    return any(ss.pos() == 'v' for ss in wn.synsets(word))\n",
    "\n",
    "def is_adjective(word):\n",
    "    # Check if the word has any adjective synsets in WordNet\n",
    "    return any(ss.pos() == 'a' or ss.pos() == 's' for ss in wn.synsets(word))\n",
    "\n",
    "def is_adverb(word):\n",
    "    # Check if the word has any adverb synsets in WordNet\n",
    "    return any(ss.pos() == 'r' for ss in wn.synsets(word))\n",
    "\n",
    "\n",
    "def get_list_forms(word, pos):\n",
    "    doc = nlp(word)\n",
    "    token = doc[0]\n",
    "\n",
    "    base_form = token.text\n",
    "    comparative_form = token._.inflect(f'JJR' if pos == 'JJ' else 'RBR') if token._.inflect(f'JJR' if pos == 'JJ' else 'RBR') else word\n",
    "    superlative_form = token._.inflect(f'JJS' if pos == 'JJ' else 'RBS') if token._.inflect(f'JJS' if pos == 'JJ' else 'RBS') else word\n",
    "    \n",
    "    return base_form, comparative_form, superlative_form\n",
    "    \n",
    "\n",
    "def get_list_noun_forms(word):\n",
    "    doc = nlp(word)\n",
    "    token = doc[0]\n",
    "\n",
    "    singular_form = token._.inflect('NN') if token._.inflect('NN') else word\n",
    "    plural_form = token._.inflect('NNS') if token._.inflect('NNS') else word\n",
    "    \n",
    "    return singular_form, plural_form\n",
    "    \n",
    "\n",
    "def get_list_verb_forms(word):\n",
    "    base_form = lemmatizer.lemmatize(word, 'v')\n",
    "    \n",
    "    return base_form,conjugate_verb(base_form, 'VBD'),conjugate_verb(base_form, 'VBG'),conjugate_verb(base_form, 'VBN'),base_form, conjugate_verb(base_form, 'VBZ')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_forms(word, pos):\n",
    "    doc = nlp(word)\n",
    "    token = doc[0]\n",
    "\n",
    "    base_form = token.text\n",
    "    comparative_form = token._.inflect(f'JJR' if pos == 'JJ' else 'RBR') if token._.inflect(f'JJR' if pos == 'JJ' else 'RBR') else word\n",
    "    superlative_form = token._.inflect(f'JJS' if pos == 'JJ' else 'RBS') if token._.inflect(f'JJS' if pos == 'JJ' else 'RBS') else word\n",
    "    \n",
    "    return {\n",
    "        'base_form': base_form,\n",
    "        'comparative_form': comparative_form,\n",
    "        'superlative_form': superlative_form\n",
    "    }\n",
    "\n",
    "def get_noun_forms(word):\n",
    "    doc = nlp(word)\n",
    "    token = doc[0]\n",
    "\n",
    "    singular_form = token._.inflect('NN') if token._.inflect('NN') else word\n",
    "    plural_form = token._.inflect('NNS') if token._.inflect('NNS') else word\n",
    "    \n",
    "    return {\n",
    "        'singular_form': singular_form,\n",
    "        'plural_form': plural_form\n",
    "    }\n",
    "\n",
    "def get_verb_forms(word):\n",
    "    base_form = lemmatizer.lemmatize(word, 'v')\n",
    "    \n",
    "    return {\n",
    "        'base_form': base_form,\n",
    "        'past_tense': conjugate_verb(base_form, 'VBD'),\n",
    "        'gerund_or_present_participle': conjugate_verb(base_form, 'VBG'),\n",
    "        'past_participle': conjugate_verb(base_form, 'VBN'),\n",
    "        'non_3rd_person_singular_present': base_form,\n",
    "        '3rd_person_singular_present': conjugate_verb(base_form, 'VBZ'),\n",
    "    }\n",
    "\n",
    "def conjugate_verb(base, tense):\n",
    "    doc = nlp(base)\n",
    "    token = doc[0]\n",
    "    if tense == 'VBD':\n",
    "        return token._.inflect(\"VBD\")\n",
    "    elif tense == 'VBG':\n",
    "        return token._.inflect(\"VBG\")\n",
    "    elif tense == 'VBN':\n",
    "        return token._.inflect(\"VBN\")\n",
    "    elif tense == 'VBZ':\n",
    "        return token._.inflect(\"VBZ\")\n",
    "    return base\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_unique_list(input_str):\n",
    "    \"\"\"\n",
    "    Converts a string representation of a list of tuples to a list of unique values,\n",
    "    removing any 'None' values.\n",
    "    \n",
    "    Args:\n",
    "        input_str (str): The input string to be processed.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of unique values.\n",
    "    \"\"\"\n",
    "    # Remove the parentheses and quotes\n",
    "    cleaned_str = re.sub(r'[()\"]', '', input_str)\n",
    "    # Split the string into a list of tuples\n",
    "    tuples = [tuple(x.strip().split(', ')) for x in cleaned_str.split('), (')]\n",
    "    # print(\"tuples1: \", tuples)\n",
    "    \n",
    "    # Flatten the list of tuples and remove duplicates\n",
    "    result = list(set([item for tup in tuples for item in tup]))\n",
    "    \n",
    "    # Remove any 'None' values\n",
    "    result = [x for x in result if x != 'None']\n",
    "\n",
    "    result = [item.strip(\"'\") for item in result]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "word = \"be\"\n",
    "# word_forms = get_word_forms(word)\n",
    "word_forms = get_word_forms_(word)\n",
    "print(word_forms)\n",
    "print(convert_to_unique_list(str(word_forms)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  ['Spring up', 'Comprise', 'Flight', 'Cat', 'Dog', 'oddity']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def string_to_random_list(input_string):\n",
    "    \"\"\"\n",
    "    Takes a comma-separated string as input and returns a list of strings in random order.\n",
    "    \"\"\"\n",
    "    word_list = [word.strip() for word in input_string.split(',')]\n",
    "    random.shuffle(word_list)\n",
    "    return list(set(word_list))\n",
    "\n",
    "def check_words_in_text(text, words):\n",
    "    if type(words) is not list:\n",
    "        words = [word.strip() for word in words.split(',')]\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text_words = [word.lower() for word in text.split()]\n",
    "    \n",
    "    missing_words = []\n",
    "    for word in words:\n",
    "        if word not in text_words:\n",
    "            missing_words.append(word)\n",
    "    \n",
    "    return missing_words\n",
    "\n",
    "# Example usage\n",
    "story_text = \"The quick brown fox jumps over the lazy dog. It **sprang up** and **comprised**. **oddities**\"\n",
    "words_list = \"Spring up, Comprise, Flight, Cat, Dog, oddity\"\n",
    "\n",
    "missing_words = check_words_in_text(story_text, words_list)\n",
    "print(\"result: \", missing_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     --------------------------------------- 12.8/12.8 MB 12.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.64.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.1)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.0 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.13.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\usabu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: C:\\Users\\usabu\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Let's create a function to check if all provided words are included in the text\n",
    "\n",
    "import random\n",
    "import spacy\n",
    "import pyinflect\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Ensure the required nltk data is downloaded\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_word_forms_(word):\n",
    "    words = classify_words(word)\n",
    "\n",
    "    return words\n",
    "\n",
    "def classify_words(word):\n",
    "    noun_forms = None\n",
    "    verb_forms = None\n",
    "    adjective_forms = None\n",
    "    adverb_forms = None\n",
    "\n",
    "    if is_adjective(word):\n",
    "        adjective_forms = get_list_forms(word, 'JJ')\n",
    "    \n",
    "    if is_adverb(word):\n",
    "        adverb_forms = get_list_forms(word, 'RB')\n",
    "\n",
    "    if is_noun(word):\n",
    "        noun_forms = get_list_noun_forms(word)\n",
    "    \n",
    "    if is_verb(word):\n",
    "        verb_forms = get_list_verb_forms(word)\n",
    "\n",
    "    return noun_forms, verb_forms, adjective_forms, adverb_forms\n",
    "\n",
    "def is_noun(word):\n",
    "    # Check if the word has any noun synsets in WordNet\n",
    "    return any(ss.pos() == 'n' for ss in wn.synsets(word))\n",
    "\n",
    "def is_verb(word):\n",
    "    # Check if the word has any verb synsets in WordNet\n",
    "    return any(ss.pos() == 'v' for ss in wn.synsets(word))\n",
    "\n",
    "def is_adjective(word):\n",
    "    # Check if the word has any adjective synsets in WordNet\n",
    "    return any(ss.pos() == 'a' or ss.pos() == 's' for ss in wn.synsets(word))\n",
    "\n",
    "def is_adverb(word):\n",
    "    # Check if the word has any adverb synsets in WordNet\n",
    "    return any(ss.pos() == 'r' for ss in wn.synsets(word))\n",
    "\n",
    "\n",
    "def get_list_forms(word, pos):\n",
    "    doc = nlp(word)\n",
    "    token = doc[0]\n",
    "\n",
    "    base_form = token.text\n",
    "    comparative_form = token._.inflect(f'JJR' if pos == 'JJ' else 'RBR') if token._.inflect(f'JJR' if pos == 'JJ' else 'RBR') else word\n",
    "    superlative_form = token._.inflect(f'JJS' if pos == 'JJ' else 'RBS') if token._.inflect(f'JJS' if pos == 'JJ' else 'RBS') else word\n",
    "    \n",
    "    return base_form, comparative_form, superlative_form\n",
    "    \n",
    "\n",
    "def get_list_noun_forms(word):\n",
    "    doc = nlp(word)\n",
    "    token = doc[0]\n",
    "\n",
    "    singular_form = token._.inflect('NN') if token._.inflect('NN') else word\n",
    "    plural_form = token._.inflect('NNS') if token._.inflect('NNS') else word\n",
    "    \n",
    "    return singular_form, plural_form\n",
    "    \n",
    "def get_list_verb_forms(word):\n",
    "    if word.lower() == \"be\":\n",
    "        return 'be', 'was', 'were', 'being', 'been', 'am', 'is', 'are'\n",
    "        \n",
    "    base_form = lemmatizer.lemmatize(word, 'v')\n",
    "    \n",
    "    return base_form,conjugate_verb(base_form, 'VBD'),conjugate_verb(base_form, 'VBG'),conjugate_verb(base_form, 'VBN'),base_form, conjugate_verb(base_form, 'VBZ')\n",
    "\n",
    "def get_list_idiom_forms(word):\n",
    "    \n",
    "    first_word = word.split()[0]\n",
    "    rest_of_words = ' '.join(word.split()[1:])\n",
    "\n",
    "    if first_word.lower() == \"be\":\n",
    "        return 'be'+' '+ rest_of_words, 'was'+' '+ rest_of_words, 'were'+' '+ rest_of_words, 'being'+' '+ rest_of_words, 'been'+' '+ rest_of_words, 'am'+' '+ rest_of_words, 'is'+' '+ rest_of_words, 'are'+' '+ rest_of_words\n",
    "\n",
    "    base_form = lemmatizer.lemmatize(first_word, 'v')\n",
    "    return base_form +' '+ rest_of_words,conjugate_verb(base_form, 'VBD')+' '+ rest_of_words,conjugate_verb(base_form, 'VBG')+' '+ rest_of_words,conjugate_verb(base_form, 'VBN')+' '+ rest_of_words,base_form+' '+ rest_of_words, conjugate_verb(base_form, 'VBZ')+' '+ rest_of_words\n",
    "\n",
    "def conjugate_verb(base, tense):\n",
    "    doc = nlp(base)\n",
    "    token = doc[0]\n",
    "    if tense == 'VBD':\n",
    "        return token._.inflect(\"VBD\")\n",
    "    elif tense == 'VBG':\n",
    "        return token._.inflect(\"VBG\")\n",
    "    elif tense == 'VBN':\n",
    "        return token._.inflect(\"VBN\")\n",
    "    elif tense == 'VBZ':\n",
    "        return token._.inflect(\"VBZ\")\n",
    "    return base\n",
    "\n",
    "\n",
    "def convert_to_unique_list(input_str):\n",
    "    \"\"\"\n",
    "    Converts a string representation of a list of tuples to a list of unique values,\n",
    "    removing any 'None' values.\n",
    "    \n",
    "    Args:\n",
    "        input_str (str): The input string to be processed.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of unique values.\n",
    "    \"\"\"\n",
    "    # Remove the parentheses and quotes\n",
    "    cleaned_str = re.sub(r'[()\"]', '', input_str)\n",
    "    # Split the string into a list of tuples\n",
    "    tuples = [tuple(x.strip().split(', ')) for x in cleaned_str.split('), (')]\n",
    "    # print(\"tuples1: \", tuples)\n",
    "    \n",
    "    # Flatten the list of tuples and remove duplicates\n",
    "    result = list(set([item for tup in tuples for item in tup]))\n",
    "    \n",
    "    # Remove any 'None' values\n",
    "    result = [x for x in result if x != 'None']\n",
    "\n",
    "    result = [item.strip(\"'\") for item in result]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def string_to_random_list(input_string):\n",
    "    \"\"\"\n",
    "    Takes a comma-separated string as input and returns a list of strings in random order.\n",
    "    \"\"\"\n",
    "    word_list = [word.strip() for word in input_string.split(',')]\n",
    "    random.shuffle(word_list)\n",
    "    return list(set(word_list))\n",
    "\n",
    "def check_words_in_text(text, words):\n",
    "    if type(words) is not list:\n",
    "        words = string_to_random_list(words)\n",
    "        \n",
    "    # Convert both text and words to lowercase\n",
    "    text = text.lower()\n",
    "    words = [word.lower() for word in words]    \n",
    "    \n",
    "    # missing_words = [word for word in words if word not in text]\n",
    "    \n",
    "    for word in words :\n",
    "        word_forms = get_word_forms_(word) # from word\n",
    "        word_forms_list = convert_to_unique_list(str(word_forms))\n",
    "        if len(word_forms_list) == 0:\n",
    "            idioms_forms = None\n",
    "            word_forms_list.append(word)\n",
    "            if is_verb(word.split()[0]): \n",
    "                idioms_forms = list(get_list_idiom_forms(word))\n",
    "                for idiom in idioms_forms:\n",
    "                    word_forms_list.append(idiom)\n",
    "                word_forms_list= list(set(word_forms_list))\n",
    "        word_flag = False\n",
    "        for each_word in word_forms_list:\n",
    "            if each_word in text:\n",
    "                word_flag = True\n",
    "        if word_flag == True: pass\n",
    "        elif word_flag == False:\n",
    "            missing_words.append(word) \n",
    "\n",
    "    return missing_words\n",
    "\n",
    "\n",
    "words_list = None\n",
    "story_text = \"\"\n",
    "missing_words = []\n",
    "\n",
    "# Provided words list\n",
    "# Given story text\n",
    "story_text = \"The quick brown fox jumps over the lazy dog. It give off, are made up of, **sprang up** and **comprised**\"\n",
    "words_list = \"Spring up, Comprise, Flight, Cat, Dog, give off, be made up of\"\n",
    "\n",
    "\n",
    "# Check for missing words\n",
    "missing_words = check_words_in_text(story_text, words_list)\n",
    "print(missing_words)\n",
    "print(len(missing_words))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
